


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


marketing = pd.read_csv('marketing_campaign.csv',sep="\t")


marketing.head()


marketing.info()


# je fais un mini nettoyage des données avant


marketing.drop(['Z_CostContact','Z_Revenue'], axis=1, inplace=True)


marketing.fillna({'Income': marketing['Income'].median()}, inplace=True)


riche = marketing.loc[marketing['Income']>600000, 'ID']
print(riche)


marketing = marketing.drop(marketing[marketing['ID']==9432].index)


# on tente une première réduction de dimensions sur les données quantitatives


from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler


# On veut garder seulement les valeurs quantitatives
# On ne sélectionne que les colonnes contenant des valeurs numériques. select_dtypes permet de faire une sélection sur les colonnes 
# et include=[np.number] est l'argument qui permet de spécifier que l'on veut que des nombres.

marketing_numerique = marketing.select_dtypes(include=[np.number])


marketing_numerique.info()


# ici on enlève toutes les colonnes contenant des valeurs binaires, qui sont en fait des variables qualitatives. 
# on enlève aussi l'ID du client qui n'est pas une variable permettant de le classifier (à revoir car c'est aussi ce qui permet de l'identifier)

marketing_numerique = marketing_numerique.drop('AcceptedCmp1',axis=1)
marketing_numerique = marketing_numerique.drop('AcceptedCmp2',axis=1)
marketing_numerique = marketing_numerique.drop('AcceptedCmp3',axis=1)
marketing_numerique = marketing_numerique.drop('AcceptedCmp4',axis=1)
marketing_numerique = marketing_numerique.drop('AcceptedCmp5',axis=1)
marketing_numerique = marketing_numerique.drop('Complain',axis=1)
marketing_numerique = marketing_numerique.drop('Response',axis=1)
marketing_numerique = marketing_numerique.drop('ID',axis=1)


marketing_numerique.info()


# StandardScaler est une classe de la bibliothèque scikit-learn qui permet de normaliser (centrer réduire) les données

scaler = StandardScaler()

# On va donc centrer et réduire nos données marketing car sinon les revenus annuels biaisent trop l'analyse (entre 20 000 et 100 000 en valeur)

marketing_scaled = scaler.fit_transform(marketing_numerique)


# PCA est une classe de la bibliothèque scikit-learn qui permet d'effectuer des ACP.

pca = PCA()

# Ici, on fait fit_transform : fit calcule les composantes principales et les valeurs propres de notre ACP
# transform applique la transformation, càd projete nos données dans cet espace de dimension réduite.

marketing_pca = pca.fit_transform(marketing_scaled)

# On print la part de variance de chaque axe (valeur propre de chaque axe)

print(pca.explained_variance_ratio_)

# On trace le graphique des variances cumulées, avec un trait à 80 % de variance expliquée

plt.scatter(np.arange(16),np.cumsum(pca.explained_variance_ratio_))
plt.plot(np.arange(16),np.cumsum(pca.explained_variance_ratio_), color='blue', linestyle='-', marker='o')
plt.axhline(y=0.8, color='red', linestyle='--')
plt.show()


# Même méthode qu'avant, mais ici on met directement qu'on veut s'arrêter à 80 % de la variance

pca = PCA(0.8)
marketing_pca = pca.fit_transform(marketing_scaled)
print(pca.explained_variance_ratio_)


# on peut visualiser les deux premières dimensions

plt.figure(figsize=(8, 6))
plt.scatter(marketing_pca[:, 0], marketing_pca[:, 1], c='blue')
plt.title('PCA')
plt.xlabel('Composante principale 1')
plt.ylabel('Composante principale 2')
plt.show()


# On va effectuer une réduction de dimension sur les variables qualitatives maintenant
# Cela s'appelle une MCA (Multiple Correspondant Analysis)
# Pour l'effectuer, on a deux solutions : utiliser la classe mca de la bibliothèque Prince ou continuer d'utiliser scikit-learn
# Prince est plus adapté à la MCA puisque on peut l'affectuer directement avec une fonction mca
# Scikit learn ne peut pas l'effectuer directement, il faut encoder les données, puis faire une ACP, mais scikit est plus optimisé pour
# des gros volumes de données.
# Pour rester dans la logique du notebook et au vu du gros volume, on va rester sur scikit


# on importe la classe qui nous permettra d'encoder nos données qualitataives

from sklearn.preprocessing import OneHotEncoder


# On va isoler les données qualitatives

marketing_quali1 = marketing.select_dtypes(include=['object'])
marketing_quali1.info()


# on drop Dt_customer car c'est une date et elle ne doit pas être considéré comme qualitatif

marketing_quali1 = marketing_quali1.drop('Dt_Customer',axis=1)


# on isole toutes les données binaires

marketing_quali2 = [col for col in marketing.columns if marketing[col].nunique() == 2 and set(marketing[col].dropna().unique()) <= {0, 1}]

# on transforme en dataframe

marketing_quali2 = marketing[marketing_quali2]


# on ajoute les deux colonnes pour se retrouver qu'avec les variables quali

marketing_quali = pd.concat([marketing_quali1, marketing_quali2], axis=1)
marketing_quali.info()


# on initialise notre classe encoder

encoder = OneHotEncoder()

# le fit_transform nous retourne une matrice sparse, pas manipulable par les fonctions d'analyse de donnés. On la transforme donc en tableau
# avec .toarray() pour pouvoir le manipuler. 
# Cela peut être risqué car si on a trop de données car le tableau prendrait énormément d'espace en mémoire comparé au sparse, mais ici 
# ça devrait aller

marketing_encoded = encoder.fit_transform(marketing_quali).toarray()

# Que fait encoder? marketing_encoded est composé de 0 et de 1. Les colonnes de ce tableau sont toutes les modalités de toutes les variables.
# Les lignes sont les individus
# Dans chaque case, on met 1 si l'individu à cette modalité, 0 sinon


# On applique une ACP sur nos données encodées (ce qui équivaut à une MCA)

pca = PCA(0.8)
marketing_mca = pca.fit_transform(marketing_encoded)
print(pca.explained_variance_ratio_)


# on peut visualiser les deux premières dimensions

plt.figure(figsize=(8, 6))
plt.scatter(marketing_mca[:, 0], marketing_mca[:, 1], c='blue')
plt.title('MCA')
plt.xlabel('Composante principale 1')
plt.ylabel('Composante principale 2')
plt.show()


# On peut essayer de classifier en utilisant un K-means


# On essaye la méthode du coude pour obtenir le nombre de clusters optimaux

wcss = []

# On teste pour différents nombres de clusters k

for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(marketing_pca)
    wcss.append(kmeans.inertia_)

# On trace le graphique du coude

plt.plot(range(1, 10), wcss)
plt.title("Méthode du coude")
plt.xlabel("Nombre de clusters (k)")
plt.ylabel("WCSS (inertie intra-cluster)")
plt.show()


# On applique le K-Means sur nos données projetées dans l'espace de l'ACP (de dimension 3)

kmeans = KMeans(n_clusters=3, random_state=42)  # Choisir le nombre de clusters (ici 3)
clusters_k = kmeans.fit_predict(marketing_pca)  # Effectuer le clustering

plt.scatter(marketing_pca[:, 0], marketing_pca[:, 1], c=clusters_k, cmap='viridis')
plt.title("Clustering K-Means sur les Composantes Principales 1 et 2")
plt.xlabel("Composante principale 1")
plt.ylabel("Composante principale 2")
plt.show()

plt.scatter(marketing_pca[:, 0], marketing_pca[:, 2], c=clusters_k, cmap='viridis')
plt.title("Clustering K-Means sur les Composantes Principales 1 et 3")
plt.xlabel("Composante principale 1")
plt.ylabel("Composante principale 3")
plt.show()

plt.scatter(marketing_pca[:, 1], marketing_pca[:, 2], c=clusters_k, cmap='viridis')
plt.title("Clustering K-Means sur les Composantes Principales 2 et 3")
plt.xlabel("Composante principale 2")
plt.ylabel("Composante principale 3")
plt.show()


# On va tenter une classification en Hierarchical Agglomerative Clustering (HAC)


from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch


# On utilise la classe ac de la libraire scikit-learn

ac = AgglomerativeClustering(linkage="ward", compute_distances=True)
clusters_h = ac.fit_predict(marketing_pca)


# On va utiliser la méthode du coude pour déterminer le nombre de clusters optimal

# Ceci est un tableau de taille n-1 (si on a n données) qui correspond à la distance de fusion
# des différents clusters. Cela permet de visualiser comment les clusters se rapprochent au fil des itérations

distances = ac.distances_

# On trace un graphique qui représente le nombre de clusters en fonction de la distance

n_sizes = 20
x = np.arange(n_sizes, 0, -1)
y = ac.distances_[-n_sizes:]

plt.scatter(x, y)

plt.xlabel('Point indices')
plt.ylabel('Distances')
plt.title("Choice of the number of classes")
plt.show()


# On voit donc qu'après 3 clusters, la distance de fusion entre clusters augmente fortement
# On en déduit que 3 clusters est un nombre optimal

# On fait un dendogramme pour voir la construction des clusters de la méthode HAC
# Chaque point de donnée commence dans son propre cluster. Ainsi, si on a n points de données, on commence avec n clusters
# L'algorithme identifie les deux clusters les plus proches et les fusionne pour former un nouveau cluster
# Ce processus de fusion est basé sur la proximité entre les clusters, et il peut être mesuré de différentes manières que l'on verra après
# Après chaque fusion de deux clusters, le nombre total de clusters diminue de un 
# Ce processus est répété jusqu'à ce qu'il ne reste plus qu’un seul cluster ou jusqu'à ce que l’on décide d’arrêter à un nombre de clusters prédéfini


# On trace le dendogramme 

K = 3

ac = AgglomerativeClustering(n_clusters=K, compute_distances=True, linkage='ward')
clusters_h = ac.fit_predict(marketing_pca)

children = ac.children_
distances = ac.distances_
n_observations = np.arange(2, children.shape[0]+2)
linkage_matrix = np.c_[children, distances, n_observations]

# Cutting the dendrogram to get K classes
max_d = .5*(ac.distances_[-K]+ac.distances_[-K+1])
plt.axhline(y=max_d, c='k')

sch.dendrogram(linkage_matrix, color_threshold=max_d, labels=ac.labels_)

plt.title("Dendrogram with Ward linkage")
plt.show()


# Le dendogramme se construit donc de bas en haut. La hauteur à laquelle se fait une fusion indique la similitude entre les deux
# clusters fusionnés. Autrement dit, plus la fusion est haute, moins les deux clusters fusionnés se ressemblent
# C'est à nous de déterminer ensuite quel est le nombre de clusters optimal, en équilibrant entre nombre de clusters et homogénéité des clusters
# On tire un trait horizontal pour déterminer le nombre de clusters


# On visualise les individus classés dans l'espace de l'acp

colors = ['red', 'green', 'orange']

plt.scatter(marketing_pca[:, 0], marketing_pca[:, 1], c=[colors[i-1] for i in clusters_h])
plt.title('Scatter Plot - Clustering Hiérarchique')
plt.xlabel('Composante principale 1')
plt.ylabel('Composante principale 2')
plt.show()

plt.scatter(marketing_pca[:, 0], marketing_pca[:, 2], c=[colors[i-1] for i in clusters_h])
plt.title('Scatter Plot - Clustering Hiérarchique')
plt.xlabel('Composante principale 1')
plt.ylabel('Composante principale 3')
plt.show()

plt.scatter(marketing_pca[:, 1], marketing_pca[:, 2], c=[colors[i-1] for i in clusters_h])
plt.title('Scatter Plot - Clustering Hiérarchique')
plt.xlabel('Composante principale 2')
plt.ylabel('Composante principale 3')
plt.show()


# On va comparer nos classifications

fig, axs = plt.subplots(3, 2, figsize=(15, 10)) 

axs[0,0].scatter(marketing_pca[:, 0], marketing_pca[:, 1], c=clusters_k, cmap='viridis')
axs[0,0].set_title("K-means - x = Dim 1, y =  Dim2")

axs[0,1].scatter(marketing_pca[:, 0], marketing_pca[:, 1], c=[colors[i-1] for i in clusters_h])
axs[0,1].set_title("HAC - x = Dim1 , y = Dim2")

axs[1,0].scatter(marketing_pca[:, 0], marketing_pca[:, 2], c=clusters_k, cmap='viridis')
axs[1,0].set_title("K-means - x=Dim 1, y=Dim3")

axs[1,1].scatter(marketing_pca[:, 0], marketing_pca[:, 2], c=[colors[i-1] for i in clusters_h])
axs[1,1].set_title("HAC - x = Dim1 , y = Dim3")

axs[2,0].scatter(marketing_pca[:, 1], marketing_pca[:, 2], c=clusters_k, cmap='viridis')
axs[2,0].set_title("K-means - x=Dim 2, y=Dim3")

axs[2,1].scatter(marketing_pca[:, 1], marketing_pca[:, 2], c=[colors[i-1] for i in clusters_h])
axs[2,1].set_title("HAC - x = Dim2 , y = Dim3")

plt.tight_layout()
plt.show()


# Allez, on passe au Gaussian Mixture Model! (GMM)


from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score 


# On utilise la classe GaussianMixture de scikit-learn

K = 3

gmm = GaussianMixture(n_components=K, n_init=10, random_state=42)
clusters_gmm = gmm.fit_predict(marketing_pca)


# On va visulaliser les résultats

colors_gmm = ['blue', 'magenta', 'black']

plt.scatter(marketing_pca[:, 0], marketing_pca[:, 1], c=[colors_gmm[i-1] for i in clusters_gmm])
plt.title('Scatter Plot - GMM')
plt.xlabel('Composante principale 1')
plt.ylabel('Composante principale 2')
plt.show()

plt.scatter(marketing_pca[:, 0], marketing_pca[:, 2], c=[colors_gmm[i-1] for i in clusters_gmm])
plt.title('Scatter Plot - GMM')
plt.xlabel('Composante principale 1')
plt.ylabel('Composante principale 3')
plt.show()

plt.scatter(marketing_pca[:, 1], marketing_pca[:, 2], c=[colors_gmm[i-1] for i in clusters_gmm])
plt.title('Scatter Plot - GMM')
plt.xlabel('Composante principale 2')
plt.ylabel('Composante principale 3')
plt.show()


# On va comparer nos classifications

fig, axs = plt.subplots(3, 3, figsize=(15, 10)) 

axs[0,0].scatter(marketing_pca[:, 0], marketing_pca[:, 1], c=clusters_k, cmap='viridis')
axs[0,0].set_title("K-means - x = Dim 1, y =  Dim2")

axs[0,1].scatter(marketing_pca[:, 0], marketing_pca[:, 1], c=[colors[i-1] for i in clusters_h])
axs[0,1].set_title("HAC - x = Dim1 , y = Dim2")

axs[0,2].scatter(marketing_pca[:, 0], marketing_pca[:, 1], c=[colors_gmm[i-1] for i in clusters_gmm])
axs[0,2].set_title("GMM - x = Dim1 , y = Dim2")

axs[1,0].scatter(marketing_pca[:, 0], marketing_pca[:, 2], c=clusters_k, cmap='viridis')
axs[1,0].set_title("K-means - x=Dim 1, y=Dim3")

axs[1,1].scatter(marketing_pca[:, 0], marketing_pca[:, 2], c=[colors[i-1] for i in clusters_h])
axs[1,1].set_title("HAC - x = Dim1 , y = Dim3")

axs[1,2].scatter(marketing_pca[:, 0], marketing_pca[:, 2], c=[colors_gmm[i-1] for i in clusters_gmm])
axs[1,2].set_title("GMM - x = Dim1 , y = Dim3")

axs[2,0].scatter(marketing_pca[:, 1], marketing_pca[:, 2], c=clusters_k, cmap='viridis')
axs[2,0].set_title("K-means - x=Dim 2, y=Dim3")

axs[2,1].scatter(marketing_pca[:, 1], marketing_pca[:, 2], c=[colors[i-1] for i in clusters_h])
axs[2,1].set_title("HAC - x = Dim2 , y = Dim3")

axs[2,2].scatter(marketing_pca[:, 1], marketing_pca[:, 2], c=[colors_gmm[i-1] for i in clusters_gmm])
axs[2,2].set_title("GMM - x = Dim2 , y = Dim3")

plt.tight_layout()
plt.show()






