


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


marketing = pd.read_csv('marketing_campaign.csv',sep="\t")


marketing.head()


marketing.info()


marketing.drop(['Z_CostContact','Z_Revenue'], axis=1, inplace=True)


marketing.fillna({'Income': marketing['Income'].median()}, inplace=True)


#On va faire quelques plots hehehehe


sns.histplot(marketing['Year_Birth'], bins = 100)
plt.title("Histogramme des dates de naissance")
plt.xlabel("Année de naissance")
plt.ylabel("Quantité")
plt.show()


sns.histplot(marketing['Income'], bins = 100)
plt.title("Histogramme des revenus")
plt.xlabel("Revenu annuel")
plt.ylabel("Quantité")
plt.show()


riche = marketing.loc[marketing['Income']>600000, 'ID']
print(riche)


marketing = marketing.drop(marketing[marketing['ID']==9432].index)


sns.histplot(marketing['Income'], bins = 100)
plt.title("Histogramme des revenus sans l'outlier")
plt.xlabel("Revenu annuel")
plt.ylabel("Quantité")
plt.show()


marketing['Dt_Customer'] = pd.to_datetime(marketing['Dt_Customer'], format='%d-%m-%Y')
sns.histplot(marketing['Dt_Customer'], bins = 100)
plt.title("Histogramme des dates d'adhésion")
plt.xlabel("Année d'adhésion")
plt.ylabel("Quantité")
plt.xticks(rotation=45)
plt.show()


# on tente une première réduction de dimension


from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler


# On veut garder seulement les valeurs quantitatives
# On ne sélectionne que les colonnes contenant des valeurs numériques. select_dtypes permet de faire une sélection sur les colonnes 
# et include=[np.number] est l'argument qui permet de spécifier que l'on veut que des nombres.

marketing_numerique = marketing.select_dtypes(include=[np.number])


marketing_numerique.info()


# ici on enlève toutes les colonnes contenant des valeurs binaires, qui sont en fait des variables qualitatives. 
# on enlève aussi l'ID du client qui n'est pas une variable permettant de le classifier (à revoir car c'est aussi ce qui permet de l'identifier)

marketing_numerique = marketing_numerique.drop('AcceptedCmp1',axis=1)
marketing_numerique = marketing_numerique.drop('AcceptedCmp2',axis=1)
marketing_numerique = marketing_numerique.drop('AcceptedCmp3',axis=1)
marketing_numerique = marketing_numerique.drop('AcceptedCmp4',axis=1)
marketing_numerique = marketing_numerique.drop('AcceptedCmp5',axis=1)
marketing_numerique = marketing_numerique.drop('Complain',axis=1)
marketing_numerique = marketing_numerique.drop('Response',axis=1)
marketing_numerique = marketing_numerique.drop('ID',axis=1)


marketing_numerique.info()


# StandardScaler est une classe de la bibliothèque scikit-learn qui permet de normaliser (centrer réduire) les données

scaler = StandardScaler()

# On va donc centrer et réduire nos données marketing car sinon les revenus annuels biaisent trop l'analyse (entre 20 000 et 100 000 en valeur)

marketing_scaled = scaler.fit_transform(marketing_numerique)


# PCA est une classe de la bibliothèque scikit-learn qui permet d'effectuer des ACP.

pca = PCA()

# Ici, on fait fit_transform : fit calcule les composantes principales et les valeurs propres de notre ACP
# transform applique la transformation, càd projete nos données dans cet espace de dimension réduite.

marketing_pca = pca.fit_transform(marketing_scaled)

# On print la part de variance de chaque axe (valeur propre de chaque axe)

print(pca.explained_variance_ratio_)

# On trace le graphique des variances cumulées, avec un trait à 80 % de variance expliquée

plt.scatter(np.arange(16),np.cumsum(pca.explained_variance_ratio_))
plt.plot(np.arange(16),np.cumsum(pca.explained_variance_ratio_), color='blue', linestyle='-', marker='o')
plt.axhline(y=0.8, color='red', linestyle='--')
plt.show()


# Même méthode qu'avant, mais ici on met directement qu'on veut s'arrêter à 80 % de la variance

pca = PCA(0.8)
marketing_pca = pca.fit_transform(marketing_scaled)
print(pca.explained_variance_ratio_)


# On peut essayer de classifier en utilisant un K-means


# On essaye la méthode du coude pour obtenir le nombre de clusters optimaux

wcss = []

# On teste pour différents nombres de clusters k

for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(marketing_pca)
    wcss.append(kmeans.inertia_)

# On trace le graphique du coude

plt.plot(range(1, 10), wcss)
plt.title("Méthode du coude")
plt.xlabel("Nombre de clusters (k)")
plt.ylabel("WCSS (inertie intra-cluster)")
plt.show()


# On applique le K-Means sur nos données projetées dans l'espace de l'ACP (de dimension 3)

kmeans = KMeans(n_clusters=3, random_state=42)  # Choisir le nombre de clusters (ici 3)
clusters_k = kmeans.fit_predict(marketing_pca)  # Effectuer le clustering

plt.scatter(marketing_pca[:, 0], marketing_pca[:, 1], c=clusters_k, cmap='viridis')
plt.title("Clustering K-Means sur les Composantes Principales 1 et 2")
plt.xlabel("Composante principale 1")
plt.ylabel("Composante principale 2")
plt.show()

plt.scatter(marketing_pca[:, 0], marketing_pca[:, 2], c=clusters_k, cmap='viridis')
plt.title("Clustering K-Means sur les Composantes Principales 1 et 3")
plt.xlabel("Composante principale 1")
plt.ylabel("Composante principale 3")
plt.show()

plt.scatter(marketing_pca[:, 1], marketing_pca[:, 2], c=clusters_k, cmap='viridis')
plt.title("Clustering K-Means sur les Composantes Principales 2 et 3")
plt.xlabel("Composante principale 2")
plt.ylabel("Composante principale 3")
plt.show()


# On va tenter une classification en Hierarchical Agglomerative Clustering (HAC)


from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster


# On fait un dendogramme pour voir la construction des clusters de la méthode HAC
# Chaque point de donnée commence dans son propre cluster. Ainsi, si on a n points de données, on commence avec n clusters
# L'algorithme identifie les deux clusters les plus proches et les fusionne pour former un nouveau cluster
# Ce processus de fusion est basé sur la proximité entre les clusters, et il peut être mesuré de différentes manières que l'on verra après
# Après chaque fusion de deux clusters, le nombre total de clusters diminue de un 
# Ce processus est répété jusqu'à ce qu'il ne reste plus qu’un seul cluster ou jusqu'à ce que l’on décide d’arrêter à un nombre de clusters prédéfini

# La fonction linkage permet de créer un matrice de distances

Z = linkage(marketing_pca, method='ward')  # 'ward' est une des manières de calculer la proximité entre clusters

# On visualise le dendrogramme

plt.figure(figsize=(10, 7))
dendrogram(Z, color_threshold=60) # le paramètre color threshold nous permet de décider à quel hauter on coupe pour former des clusters
plt.title('Dendrogramme - Clustering Hiérarchique')
plt.ylabel('Distance')
plt.show()


# Le dendogramme se construit donc de bas en haut. La hauteur à laquelle se fait une fusion indique la similitude entre les deux
# clusters fusionnés. Autrement dit, plus la fusion est haute, moins les deux clusters fusionnés se ressemblent
# C'est à nous de déterminer ensuite quel est le nombre de clusters optimal, en équilibrant entre nombre de clusters et homogénéité des clusters
# On tire un trait horizontal pour déterminer le nombre de clusters


# Permet d'attribuer le label de son cluster à chaque point: a chaque individu est assigné 1, 2 ou 3.

clusters_h = fcluster(Z, t=3, criterion='maxclust') # criterion = maxclust permet de spécifier qu'on veut un nombre precis de clusters

# On trace un scatter plot avec les couleurs des clusters

colors = ['orange', 'green', 'red']

plt.scatter(marketing_pca[:, 0], marketing_pca[:, 1], c=[colors[i-1] for i in clusters_h])
plt.title('Scatter Plot - Clustering Hiérarchique')
plt.xlabel('Composante principale 1')
plt.ylabel('Composante principale 2')
plt.show()

plt.scatter(marketing_pca[:, 0], marketing_pca[:, 2], c=[colors[i-1] for i in clusters_h])
plt.title('Scatter Plot - Clustering Hiérarchique')
plt.xlabel('Composante principale 1')
plt.ylabel('Composante principale 3')
plt.show()

plt.scatter(marketing_pca[:, 1], marketing_pca[:, 2], c=[colors[i-1] for i in clusters_h])
plt.title('Scatter Plot - Clustering Hiérarchique')
plt.xlabel('Composante principale 2')
plt.ylabel('Composante principale 3')
plt.show()


# Note: on aurait pu aussi effectuer le HAC avec la classe hac de scikit-learn


# On va comparer nos classifications

fig, axs = plt.subplots(3, 2, figsize=(15, 10)) 

axs[0,0].scatter(marketing_pca[:, 0], marketing_pca[:, 1], c=clusters_k, cmap='viridis')
axs[0,0].set_title("K-means - x = Dim 1, y =  Dim2")

axs[0,1].scatter(marketing_pca[:, 0], marketing_pca[:, 1], c=[colors[i-1] for i in clusters_h])
axs[0,1].set_title("HAC - x = Dim1 , y = Dim2")

axs[1,0].scatter(marketing_pca[:, 0], marketing_pca[:, 2], c=clusters_k, cmap='viridis')
axs[1,0].set_title("K-means - x=Dim 1, y=Dim3")

axs[1,1].scatter(marketing_pca[:, 0], marketing_pca[:, 2], c=[colors[i-1] for i in clusters_h])
axs[1,1].set_title("HAC - x = Dim1 , y = Dim3")

axs[2,0].scatter(marketing_pca[:, 1], marketing_pca[:, 2], c=clusters_k, cmap='viridis')
axs[2,0].set_title("K-means - x=Dim 2, y=Dim3")

axs[2,1].scatter(marketing_pca[:, 1], marketing_pca[:, 2], c=[colors[i-1] for i in clusters_h])
axs[2,1].set_title("HAC - x = Dim2 , y = Dim3")

plt.tight_layout()
plt.show()



